# Reddit Scraper - r/datascience

**Autores:** Jordi Guillem y Xairo Campos  
**Asignatura:** M2.851 - TipologÃ­a y ciclo de vida de los datos  
**PrÃ¡ctica:** 2 - Data Cleaning  
**Universidad:** UOC  
**Fecha:** Diciembre 2025

---

## DescripciÃ³n

Proyecto completo de anÃ¡lisis de datos sobre posts del subreddit r/datascience. El proyecto incluye:

1. **PrÃ¡ctica 1:** Web scraping de posts del subreddit
2. **PrÃ¡ctica 2:** Limpieza, integraciÃ³n y preparaciÃ³n de datos
3. **AnÃ¡lisis avanzado:** Modelado supervisado, no supervisado y contraste de hipÃ³tesis

El dataset contiene posts del subreddit r/datascience: "A space for data science professionals to engage in discussions and debates on the subject of data science." El anÃ¡lisis identifica patrones de engagement, caracterÃ­sticas de posts exitosos y clusters temÃ¡ticos. 

---

## Estructura del proyecto

```
M2.851-PRACT2/
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ processed
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â”œâ”€â”€ reddit_datascience_dataset.csv      # Dataset principal (960 posts)
â”‚   â”‚   â”œâ”€â”€ reddit_datascience_extradata.csv    # Dataset con datos de upvote y permalink
â”œâ”€â”€ output
â”‚   â”œâ”€â”€ reddit_datascience_clean.csv            # Dataset limpio y procesado (960 posts Ã— 24 vars)
â”œâ”€â”€ source
â”‚   â”œâ”€â”€ analyze_dataset.py                      # Script para analizar el dataset generado
â”‚   â”œâ”€â”€ clean_after_integration.py              # MÃ³dulo para la imputaciÃ³n de datos faltantes y tipificaciÃ³n
â”‚   â”œâ”€â”€ config.py                               # ConfiguraciÃ³n del pipeline de limpieza y anÃ¡lisis
â”‚   â”œâ”€â”€ integrate_data.py                       # MÃ³dulo para la integraciÃ³n de los diferentes datasets
â”‚   â”œâ”€â”€ load_data.py                            # MÃ³dulo para la carga de los diferentes datasets
â”‚   â”œâ”€â”€ main.py                                 # Script principal de limpieza
â”‚   â”œâ”€â”€ outliers.py                             # DetecciÃ³n y marcado de outliers (IQR)
â”‚   â”œâ”€â”€ select_columns.py                       # MÃ³dulo para la selecciÃ³n de los campos
â”‚   â”œâ”€â”€ utils.py                                # Funciones auxiliares
â”œâ”€â”€ analisis_reddit_datascience.ipynb           # ğŸ“Š NOTEBOOK DE ANÃLISIS COMPLETO
â”œâ”€â”€ .gitignore
â”œâ”€â”€ M2.851_20251_PrÃ¡ctica2.pdf                  # Enunciado de la prÃ¡ctica
â”œâ”€â”€ memoria.txt                                 # Enlace a Google Drive con documentaciÃ³n adicional
â”œâ”€â”€ README.md                                   # Este archivo
â”œâ”€â”€ requirements.txt                            # Dependencias del proyecto

```

## InstalaciÃ³n

### Requisitos previos
- Python 3.8 o superior
- pip (gestor de paquetes de Python)

### DefiniciÃ³n del entorno virutal e Instalar dependencias

```bash
python -m venv env
source env/bin/activate
pip install -r requirements.txt
```

Esto instalarÃ¡ las librerÃ­as:

- **pandas**: ManipulaciÃ³n y anÃ¡lisis de datos
- **numpy**: ComputaciÃ³n numÃ©rica
- **matplotlib**: VisualizaciÃ³n de datos (grÃ¡ficos)
- **seaborn**: VisualizaciÃ³n estadÃ­stica avanzada
- **scikit-learn**: Machine learning (Random Forest, K-Means, PCA, mÃ©tricas)
- **scipy**: Tests estadÃ­sticos (Shapiro-Wilk, Levene, t-test, Mann-Whitney)

## Uso

### 1. Ejecutar el pipeline de integraciÃ³n, limpieza y anÃ¡lisis

Para ejecutar el script principal de integraciÃ³n, limpieza y anÃ¡lisis 

```bash
cd source
python main.py
```

SaldrÃ¡ por el prompt de comandos los diferentes pasos del pipeline del proyecto:

1. Cargando datasets
2. Limpieza bÃ¡sica del dataset original
3. Integrando datasets
RESUMEN TRAS INTEGRACIÃ“N Y FILTRADO
4. Limpieza avanzada
5. SelecciÃ³n de columnas finales
6. Guardando dataset limpio

### 2. Analizar el dataset generado

Una vez generado el dataset:

```bash
cd source
python analyze_dataset.py
```

Este script mostrarÃ¡:
- Total de posts extraÃ­dos
- EstadÃ­sticas de karma y comentarios
- DistribuciÃ³n de sentimientos
- Tipos de contenido
- Autores mÃ¡s activos
- Flairs mÃ¡s comunes
- Top posts con mÃ¡s engagement

### 3. Ejecutar el notebook de anÃ¡lisis completo

El archivo **`analisis_reddit_datascience.ipynb`** contiene el anÃ¡lisis completo del proyecto:

**Contenido del notebook:**

1. **DescripciÃ³n del Dataset:** Contexto, variables y motivaciÃ³n del estudio
2. **IntegraciÃ³n y SelecciÃ³n:** Resumen del proceso de preparaciÃ³n de datos
3. **Limpieza de Datos:** VerificaciÃ³n de imputaciÃ³n, tipificaciÃ³n y gestiÃ³n de outliers
4. **AnÃ¡lisis de Datos:**
   - **Modelo Supervisado (Random Forest):** ClasificaciÃ³n de posts de alto engagement
   - **Modelo No Supervisado (K-Means):** Clustering de posts por caracterÃ­sticas
   - **Contraste de HipÃ³tesis:** Test estadÃ­stico sobre sentimiento vs engagement
5. **Visualizaciones:** GrÃ¡ficos de distribuciones, correlaciones, ROC, clusters, etc.
6. **Conclusiones:** InterpretaciÃ³n de resultados y resoluciÃ³n del problema

**Para ejecutar el notebook:**

```bash
# Instalar Jupyter si no lo tienes
pip install jupyter

# Lanzar Jupyter Notebook
jupyter notebook analisis_reddit_datascience.ipynb
```

O abrir directamente el archivo `.ipynb` en **VS Code** con la extensiÃ³n de Jupyter instalada.

**CaracterÃ­sticas destacadas del anÃ¡lisis:**
- âœ… Modelos de machine learning supervisado y no supervisado
- âœ… VerificaciÃ³n de supuestos estadÃ­sticos (normalidad, homocedasticidad)
- âœ… Visualizaciones profesionales con matplotlib y seaborn
- âœ… Interpretaciones detalladas de cada resultado
- âœ… CÃ³digo ejecutable y reproducible

**Resultados principales obtenidos:**

ğŸ“Š **Modelo Supervisado (Random Forest):**
- Accuracy: 91.67%
- ROC-AUC: 0.9816
- Top predictor: nÃºmero de comentarios (44.28% importancia)
- Segundo predictor: upvote ratio (39.20% importancia)

ğŸ” **Modelo No Supervisado (K-Means):**
- 4 clusters identificados
- Cluster 0: Posts de bajo engagement controversial (138 posts)
- Cluster 1: Posts estÃ¡ndar positivos (507 posts)
- Cluster 2: Posts con sentimiento negativo/neutral (287 posts)
- Cluster 3: Posts virales (28 posts, karma promedio: 1429.71)

ğŸ“ˆ **Contraste de HipÃ³tesis (Mann-Whitney U):**
- p-value: 0.017779 (< 0.05)
- DecisiÃ³n: Se rechaza Hâ‚€
- ConclusiÃ³n: Existe diferencia estadÃ­sticamente significativa en el karma entre posts positivos y no-positivos
- TamaÃ±o del efecto: -0.1636 (efecto pequeÃ±o)

---

## ConfiguraciÃ³n


### ParÃ¡metros principales:

- **`OUTPUT_FILE`**: Ruta del archivo CSV de salida
- **`ORIGINAL_DATASET`**: Nombre del archivo csv del dataset principal 
- **`EXTRA_DATASET`**: Nombre del archivo csv del dataset con datos adicionales
- **`CLEAN_OUTPUT_FILENAME`**: Nombre del archivo csv de los datos limpios del pipeline
---

## Resumen de las consideraciones de limpieza

| Campo              | ImputaciÃ³n                                   | Resumen                                           |
|--------------------|-----------------------------------------------|---------------------------------------------------|
| title              | "untitled"                                    | TÃ­tulo desconocido o vacÃ­o                        |
| author             | "unknown"                                     | Autor no disponible                               |
| karma              | Media redondeada                              | Mantiene distribuciÃ³n original                    |
| upvote_ratio_new   | Media redondeada                              | Evita sesgos por valores faltantes                |
| num_comments       | 0                                             | Sin comentarios registrados                       |
| flair              | "no flair"                                    | PublicaciÃ³n sin etiqueta                          |
| content_type       | "text" si hay contenido, si no "unknown"      | Inferido a partir de text_content                 |
| text_content       | "no content"                                  | No hay texto disponible                           |
| media_url          | "no media url"                                | No contiene medios                                |
| external_url       | "no external url"                             | No enlaza a recursos externos                     |
| posted_time        | 1970-01-01                                    | Fecha tÃ©cnica para evitar NaT                     |
| posted_hour        | -1                                            | Hora desconocida                                  |
| sentiment          | "neutral"                                     | Valor por defecto                                 |
| sentiment_score    | 0                                             | Sentimiento neutral                               |
| sentiment_positive | 0                                             | NormalizaciÃ³n posterior                           |
| sentiment_negative | 0                                             | NormalizaciÃ³n posterior                           |
| sentiment_neutral  | 1                                             | NormalizaciÃ³n posterior                           |
| scraped_at         | 1970-01-01                                    | Fecha tÃ©cnica si falta                            |
| post_id            | "no post id"                                  | Identificador ausente                             |
| permalink          | URL generada                                  | https://old.reddit.com/r/datascience/<post_id_sin_prefijo> |)

---

## TecnologÃ­as utilizadas

- **Python 3.8+**: Lenguaje de programaciÃ³n
- **Pandas**: Procesamiento y anÃ¡lisis de datos
- **NumPy**: ComputaciÃ³n numÃ©rica
- **Matplotlib & Seaborn**: VisualizaciÃ³n de datos
- **Scikit-learn**: Machine learning (Random Forest, K-Means, mÃ©tricas)
- **SciPy**: Tests estadÃ­sticos (Shapiro-Wilk, Levene, t-test, Mann-Whitney)
- **Jupyter Notebook**: AnÃ¡lisis interactivo y documentaciÃ³n

---


## Contacto

**Jordi Guillem** | **Xairo Campos**  
Universidad Oberta de Catalunya (UOC)  
M2.851 - TipologÃ­a y ciclo de vida de los datos  
PrÃ¡ctica 2 - Data Cleaning
Diciembre 2025
